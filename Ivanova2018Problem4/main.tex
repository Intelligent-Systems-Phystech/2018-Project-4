\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1, T2A]{fontenc}

\input xy
\xyoption{all}
\newtheorem{Def}{Определение}
\renewcommand{\abstractname}{\textbf{Анотация}:}
\newcommand{\fD}{\mathfrak{D}}
\date{October 2018}
\usepackage{natbib}
\usepackage{graphicx}
\begin{document}
\title{Автоматическая настройка параметров АРТМ под широкий класс задач}
\date{}
\maketitle
\begin{center}
   {Е.\,В.~Иванова\footnote{Московский физико-технический институт, ivanova.ev@phystech.edu},
   C.\,Н.~Матвеева\footnote{Московский физико-технический институт, matveeva.sn@phystech.edu},
   Т.\,А.~Голубева\footnote{Московский физико-технический институт, golubeva.ta@phystech.edu,},
   А.\,В.~Трусов\footnote{Московский физико-технический институт, trusov.av@phystech.edu,},
   В.\,В.~Черноног\footnote{Московский физико-технический институт, chernonog.vv@phystech.edu,},
   М.\,В.~Царицын\footnote{Московский физико-технический институт, tsaritsyn.mv@phystech.edu,}
   }
\end{center}


\textbf{Аннотация:} В работе рассматривается задача тематического моделирования. Тематическое моделирование нашло своё применение в таких областях как машинное обучение и обработка естественного языка. В работе используется широко известная библиотека BigARTM, использование которой требует настройки большого числа параметров. В работе рассматривается возможность нахождения универсального набора значений параметров для широкого класса задач. Для нахождения этих значений, используется метод <...>. Для оценки качества используется критерий <...>. Полученная, с помощью подобранных коэффициентов, ошибка не больше чем на <X>\% больше чем в "локально лучших моделях".


\bigskip
\textbf{Ключевые слова}: тематической моделирование, ARTM, BigARTM, глобальная оптимизация.

\section {Введение}
Тематическое моделирование — способ построения модели коллекции текстовых документов, которая определяет, к каким темам относится каждый из документов\cite{wallach2006topic}. Тематическая модель (англ. topic model) коллекции текстовых документов определяет, к каким темам относится каждый документ и какие слова  образуют каждую тему.
Тематические модели широко используются на практике для решения задач классификации и ранжирования документов, а также для разведочного поиска\cite{janina2016}.  
 Открытая библиотека bigARTM\cite{vorontsov2015bigartm}  позволяет строить тематические модели, используя широкий класс возможных регуляризаторов\cite{vorontsov2013veroyat}.  Однако такая гибкость приводит к тому, что задача настройки коэффициентов оказывается очень сложной. Эту настройку можно значительно упростить, используя механизм относительных коэффициентов регуляризации и автоматический выбор N-грамм. В работе проверяется гипотеза о том, что существует универсальный набор относительных коэффициентов регуляризации, дающий "достаточно хорошие" результаты на широком классе задач. Дано несколько датасетов с внешним критерием качества (классификация документов по категориям и ранжирование). Находятся лучшие параметры для конкретного датасета, дающие "локально лучшую модель". Ищется алгоритм инициализации bigARTM, производящий тематические модели с качеством, сравнимым с "локально лучшей моделью" на её датасете. Критерий сравнимости по качеству: на данном датасете качество "универсальной модели" не более чем на 5\% хуже, чем у "локально лучшей модели".Производится сравнение с другими более простыми моделями: PLSA(Probabilistic latent semantic analysis)\cite{hofmann1999probabilistic} и LDA(Latent Dirichlet allocation)\cite{blei2003latent} 
 
\section {Постановка задачи}
\subsection{Рассматриваемые датасеты}
\subsubsection{Victorian Era Authorship Attribution Data Set}
Рассматривается датасет Victorian Era Authorship Attribution Data Set. В данном датасете рассматриваются авторы, удовлетворяющие следующим критериям: 
\begin{itemize}
\item англоязычные авторы
\item авторы, у которых не менее 5 книг 
\item авторы XIX века
\end{itemize}
Всего рассматривается около 50 авторов. В выбранных текстах были отброшены первые и последние 500 слов, чтобы избавиться от несущественной информации, такой как имя и другая информация об авторе, название книги и тд. Все тексты были изучены для получения общего числа уникальных слов и их частот. После этого из тектов взяли наиболее часто всречаемые 10000 слов, которые были пронумерованы в порядке убывания частот. 
\subsubsection{Twenty Newsgroups Data Set}
Датасет представляет собой набор из примерно 20 000 документов, разделенных почти равномерно по 20 различным группам новостей. Статьи являются типичными примерами публикаций и, таким образом, имеют заголовки, включая темы, подписи и цитируемые части других статей.
\subsection{Метрика}
В качестве меры важности слов в контексте документа используется tf-idf. 

TF — это частотность термина, которая измеряет, насколько часто термин встречается в документе. В длинных документах термин может встретиться в больших количествах. Поэтому применяют относительные частоты — делят количество раз, когда нужный термин встретился в тексте, на общее количество слов в данном тексте.
 
IDF — это обратная документная частота. Считается как логарифм от общего количества документов, делённого на количество документов, в которых встречается термин.

\subsection{Обзор тематических моделей}

\subsubsection{Постановка задачи тематического моделирования}
Рассмотрим  словарь терминов W из элементов которого складываются документы, и коллекцию D документов $d \subset W$.  Для каждого документа d известна его длина $n_{d}$ и количество $n_{dw}$ использований каждого термина w. Необходимо найти параметры вероятностной порождающей тематической модели, то есть представить вероятность появления p(w|d) слов в документе в виде:
\begin{equation}
p(w|d) = \sum\limits_{t \subset T} \phi_{wt}\Theta_{td}, 
\end{equation}
где $\phi_{wt}$= p(w|t) — вероятности терминов w в каждой теме t, $\Theta_{td}$ = p(t|d) — вероятности тем t в каждом документе d.

Порождающая модель описывает процесс построения коллекции по $\phi_{wt}$ и $\Theta_{td}$. Тематическое моделирование представляет собой обратную задачу: по наблюдаемой коллекции необходимо понять, какими распределениями $\phi_{wt}$ и $\Theta_{td}$ она могла бы быть получена.

\subsubsection{LDA}
При обработке на естественном языке скрытое распределение Дирихле (LDA) представляет собой генеративную статистическую модель, которая позволяет объяснять группы наблюдений ненаблюдаемыми группами, которые объясняют, почему некоторые части данных схожи. Например, если наблюдения представляют собой слова, собранные в документы, в нем говорится, что каждый документ представляет собой смесь небольшого количества тем и что присутствие каждого слова связано с одной из тем документа. LDA - пример тематической модели.

В LDA каждый документ может рассматриваться как смесь различных тем, где каждый документ считается набором тем, которые ему назначаются через LDA. Это идентично вероятностному латентному семантическому анализу (pLSA), за исключением того, что в LDA предполагается, что в распределении темы имеется редкий Dirichlet. Редкие Dirichlet priors кодируют интуицию, что документы охватывают только небольшой набор тем и что темы часто используют только небольшой набор слов. На практике это приводит к лучшему рассогласованию слов и более четкому присваиванию документов тем. LDA является обобщением модели pLSA, которая эквивалентна LDA при равномерном распределении Дирихле.

\subsubsection{PLSA}
Вероятностный латентный семантический анализ (PLSA), также известный как вероятностное латентное семантическое индексирование (PLSI, особенно в кругах поиска информации), является статистическим методом анализа двухмодовых и совпадающих данных. Фактически, можно получить низкоразмерное представление наблюдаемых переменных в терминах их близости к некоторым скрытым переменным, как и в латентном семантическом анализе, из которого развилась PLSA.

По сравнению со стандартным латентным семантическим анализом, который проистекает из линейной алгебры и сокращает таблицы возникновения (обычно через разложение сингулярных значений), вероятностный латентный семантический анализ основан на разложении смеси, полученном из модели скрытого класса.
В этом случае регуляризатор не используется.
\maketitle
\bibliographystyle{plain}
\bibliography{Ivanova2018Title}

\end{document}
